r""" """

from __future__ import annotations

from datetime import datetime
from pathlib import Path
from typing import Literal

import numpy as np
import torch
import wandb
import yaml
from matplotlib import pyplot as plt
from pydantic import BaseModel, ConfigDict, PositiveInt
from torch.nn import functional as F
from tqdm import tqdm, trange

from spd.experiments.ih.model import InductionModelConfig, InductionTransformer
from spd.log import logger
from spd.utils.data_utils import DatasetGeneratedDataLoader, InductionDataset
from spd.utils.general_utils import set_seed

wandb.require("core")


class InductionHeadsTrainConfig(BaseModel):
    model_config = ConfigDict(extra="forbid", frozen=True)
    wandb_project: str | None = None
    ih_model_config: InductionModelConfig
    steps: PositiveInt
    batch_size: PositiveInt
    lr: float
    weight_decay: float
    lr_schedule: Literal["cosine", "constant", "linear"] = "linear"
    seed: int = 0
    attention_maps_n_steps: PositiveInt
    prefix_window: PositiveInt


def get_run_name(
    config: InductionHeadsTrainConfig,
) -> str:
    """Generate a run name based on the config."""
    run_name = ""
    run_name += (
        f"induction_heads_v{config.ih_model_config.vocab_size}_seq{config.ih_model_config.seq_len}"
    )
    run_name += f"_heads{config.ih_model_config.n_heads}_layers{config.ih_model_config.n_layers}"
    run_name += f"_steps{config.steps}_batch{config.batch_size}_lr{config.lr}"
    if config.ih_model_config.use_ff:
        run_name += f"_dmodel{config.ih_model_config.d_model}"
        run_name += f"_ff_fanout{config.ih_model_config.ff_fanout}"
    if config.lr_schedule:
        run_name += f"_lr_schedule_{config.lr_schedule}"
    run_name += f"use_ff_{config.ih_model_config.use_ff}"
    run_name += f"use_pos_encoding_{config.ih_model_config.use_pos_encoding}"
    run_name += f"use_layer_norm_{config.ih_model_config.use_layer_norm}"
    return run_name


def linear_lr(step: int, steps: int) -> float:
    return 1 - (step / steps)


def constant_lr(*_: int) -> float:
    return 1.0


def cosine_decay_lr(step: int, steps: int) -> float:
    return np.cos(0.5 * np.pi * step / (steps - 1))


def train(
    model: InductionTransformer,
    dataloader: DatasetGeneratedDataLoader[tuple[torch.Tensor, torch.Tensor]],
    log_wandb: bool,
    steps: int,
    print_freq: int,
    lr: float,
    weight_decay: float,
    lr_schedule: Literal["linear", "cosine", "constant"],
) -> list[float]:
    hooks = []

    if lr_schedule == "linear":
        lr_schedule_fn = linear_lr
    elif lr_schedule == "cosine":
        lr_schedule_fn = cosine_decay_lr
    elif lr_schedule == "constant":
        lr_schedule_fn = constant_lr

    opt = torch.optim.AdamW(list(model.parameters()), lr=lr, weight_decay=weight_decay)
    losses = []

    data_iter = iter(dataloader)
    with trange(steps, ncols=0) as t:
        for step in t:
            step_lr = lr * lr_schedule_fn(step, steps)
            for group in opt.param_groups:
                group["lr"] = step_lr
            opt.zero_grad(set_to_none=True)
            # Labels is the token to predict with in-context learning
            batch, labels = next(data_iter)
            out = model(batch)
            loss = F.cross_entropy(
                out[:, -1, :],
                labels,
            )
            losses.append(loss.detach().cpu().item())
            loss.backward()
            opt.step()

            if hooks:
                hook_data = dict(model=model, step=step, opt=opt, error=loss, loss=loss, lr=step_lr)
                for h in hooks:
                    h(hook_data)
            if step % print_freq == 0 or (step + 1 == steps):
                tqdm.write(f"Step {step} Loss: {loss.item()}")
                t.set_postfix(
                    loss=loss.item(),
                    lr=step_lr,
                )
                if log_wandb:
                    wandb.log({"loss": loss.item(), "lr": step_lr}, step=step)

    return losses


def get_model_and_dataloader(
    config: InductionHeadsTrainConfig, device: str
) -> tuple[InductionTransformer, DatasetGeneratedDataLoader[tuple[torch.Tensor, torch.Tensor]]]:
    """
    Create the model and dataloader based on the config.
    """
    model = InductionTransformer(config.ih_model_config).to(device)

    # Create the dataset and dataloader
    dataset = InductionDataset(
        seq_len=config.ih_model_config.seq_len,
        vocab_size=config.ih_model_config.vocab_size,
        device=device,
        prefix_window=config.prefix_window,
    )

    dataloader = DatasetGeneratedDataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
    )

    return model, dataloader


def run_train(config: InductionHeadsTrainConfig, device: str) -> None:
    model, dataloader = get_model_and_dataloader(config, device)

    run_name = get_run_name(config)
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]
    out_dir = Path(__file__).parent / "out" / f"{run_name}_{timestamp}"
    out_dir.mkdir(parents=True, exist_ok=True)

    if config.wandb_project:
        wandb.init(project=config.wandb_project, name=run_name)

    # Save config
    config_path = out_dir / "ih_train_config.yaml"
    with open(config_path, "w") as f:
        yaml.dump(config.model_dump(mode="json"), f, indent=2)
    if config.wandb_project:
        wandb.save(str(config_path), base_path=out_dir, policy="now")
    logger.info(f"Saved config to {config_path}")

    loss_curve = train(
        model=model,
        dataloader=dataloader,
        log_wandb=False,
        steps=config.steps,
        print_freq=100,
        lr=config.lr,
        weight_decay=config.weight_decay,
        lr_schedule=config.lr_schedule,
    )

    plot_loss_curve(
        losses=loss_curve,
        out_dir=out_dir,
    )

    plot_attention_maps_post_training(
        model=model,
        dataloader=dataloader,
        steps=config.attention_maps_n_steps,
        out_dir=out_dir,
    )

    model_path = out_dir / "ih.pth"
    torch.save(model.state_dict(), model_path)
    if config.wandb_project:
        wandb.save(str(model_path), base_path=out_dir, policy="now")
    logger.info(f"Saved model to {model_path}")


def plot_loss_curve(
    losses: list[float],
    out_dir: Path,
) -> None:
    # Plot the loss curve
    plt.figure(figsize=(10, 5))
    plt.plot(losses, label="Training Loss")
    plt.xlabel("Steps")
    plt.ylabel("Loss")
    plt.title("Training Loss Curve")
    plt.legend()
    plt.grid()
    plt.savefig(out_dir / "train_loss_curve.png")
    plt.close()


def plot_attention_maps_post_training(
    model: InductionTransformer,
    dataloader: DatasetGeneratedDataLoader[tuple[torch.Tensor, torch.Tensor]],
    steps: int,
    out_dir: Path,
):
    model.eval()
    eval_attention_weights = []
    with torch.no_grad():
        for _ in range(0, steps):
            tokens, _ = next(iter(dataloader))
            attn_weights = model.get_attention_weights(tokens)
            eval_attention_weights.append(attn_weights)

        eval_attention_weights = torch.cat(eval_attention_weights, dim=0)

        # For each layer, for each head, plot the average and max attention weights
        avg_attn_weights = eval_attention_weights.mean(dim=0)
        max_attn_weights = eval_attention_weights.max(dim=0).values

        for layer_index in range(model.config.n_layers):
            for head_index in range(model.config.n_heads):
                avg_attn = avg_attn_weights[layer_index, head_index, :, :].cpu().numpy()
                max_attn = max_attn_weights[layer_index, head_index, :, :].cpu().numpy()

                fig, ax = plt.subplots(1, 2, figsize=(12, 6))
                assert isinstance(ax, np.ndarray), "Expected ax to be a numpy array of axes"
                ax[0].imshow(avg_attn, cmap="viridis", aspect="auto")
                ax[0].set_title(f"Layer {layer_index + 1}, Head {head_index + 1} - Avg Attention")
                ax[1].imshow(max_attn, cmap="viridis", aspect="auto")
                ax[1].set_title(f"Layer {layer_index + 1}, Head {head_index + 1} - Max Attention")
                plt.colorbar(ax[0].images[0], ax=ax[0])
                plt.colorbar(ax[1].images[0], ax=ax[1])
                plt.tight_layout()

                fig.savefig(out_dir / f"attention_layer{layer_index + 1}_head{head_index + 1}.png")
                plt.close(fig)


if __name__ == "__main__":
    seq_length = 64
    # The prefix window is the segment of the string the first induction
    # pair is guaranteed to land within.
    # We need 4 "spots" for the induction pair,
    # and 1 spot for the BOS token.
    prefix_window = 64 - 5
    device = "cuda" if torch.cuda.is_available() else "cpu"
    config = InductionHeadsTrainConfig(
        ih_model_config=InductionModelConfig(
            vocab_size=18,
            seq_len=seq_length,
            d_model=64,
            n_heads=1,
            n_layers=2,
            ff_fanout=4,
            use_ff=False,
            use_layer_norm=False,
            use_pos_encoding=True,
        ),
        wandb_project="induction_heads",
        steps=150000,
        batch_size=4,
        lr=0.001,
        weight_decay=0.01,
        lr_schedule="constant",
        seed=42,
        attention_maps_n_steps=100,
        prefix_window=prefix_window,
    )

    set_seed(config.seed)

    run_train(
        config=config,
        device=device,
    )
